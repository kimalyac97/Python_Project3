# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ v6 + Streamlit UI
- ê¸°ì¡´ v6 ë¡œì§ ê·¸ëŒ€ë¡œ ì‚¬ìš© (requests+BS4 ì „ìš©)
- ì›¹ UIì—ì„œ:
  * ê³ ê°ì‚¬ ì—‘ì…€(Aì—´) ê¸°ë°˜ ìˆ˜ì§‘ ("ê°’" +ì‚¬ê³ )  â†’ ì‹œíŠ¸ëª…: ê³ ê°ì‚¬ (A=ê²€ìƒ‰ì–´, B=ì œëª©, C=ê¸°ì‚¬ë‚´ìš©, D=ë§í¬)
  * ì‚¬ìš©ì ì§€ì • ê²€ìƒ‰ì–´(ì¤„ ë‹¨ìœ„)              â†’ ì‹œíŠ¸ëª…: ì‚¬ìš©ì ì§€ì • (A=ê²€ìƒ‰ì–´, B=ì œëª©, C=ê¸°ì‚¬ë‚´ìš©, D=ë§í¬)
  * ì „ë ¥ì‹œì¥ +ì—ë„ˆì§€                          â†’ ì‹œíŠ¸ëª…: ì „ë ¥ì‹œì¥ ë™í–¥ (A=ì œëª©, B=ê¸°ì‚¬ë‚´ìš©, C=ë§í¬)  â€»ê²€ìƒ‰ì–´ ì»¬ëŸ¼ ì—†ìŒ
  * N ìµœëŒ€ 1~10(ìœ ë‹ˆí¬)
"""

import os, re, io, time, random, logging, sys
from datetime import datetime
from urllib.parse import quote, urlparse, urlunparse, parse_qs, urlencode

import pandas as pd
import requests
from bs4 import BeautifulSoup
from openpyxl import Workbook
from openpyxl.utils import get_column_letter

import streamlit as st

# ===== ê¸°ë³¸ ì„¤ì • =====
BASE_URL = "https://search.naver.com/search.naver?ssc=tab.news.all&where=news&sm=tab_jum&query={query}"
HOMEPAGE = "https://www.naver.com/"
NAVER_QUERY_DELAY_RANGE = (0.8, 1.6)
RETRY_BACKOFF_BASE = 1.5
MAX_RETRY_DEFAULT = 2

UA_POOL = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36",
]
COMMON_HEADERS = {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.6,en;q=0.5",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Referer": HOMEPAGE,
}

# ì‹œê°„: <span class="sds-comps-text sds-comps-text-type-body2 sds-comps-text-weight-sm">1ì‹œê°„ ì „</span>
TIME_SPAN_CLASS = ["sds-comps-text", "sds-comps-text-type-body2", "sds-comps-text-weight-sm"]
SNIPPET_CLASSES = ["sds-comps-text-ellipsis", "sds-comps-text-type-body2"]
TRACKING_PARAMS = {"utm_source","utm_medium","utm_campaign","utm_term","utm_content","utm_name",
                   "gclid","fbclid","igshid","utm_id","utm_referrer","ref","sns","spm","cmpid"}

# ===== ë¡œê±°(ê°„ë‹¨) =====
logger = logging.getLogger("naver_news_streamlit")
if not logger.handlers:
    logger.setLevel(logging.INFO)
    sh = logging.StreamHandler(sys.stdout)
    sh.setLevel(logging.INFO)
    sh.setFormatter(logging.Formatter("%(asctime)s | %(levelname)s | %(message)s", "%H:%M:%S"))
    logger.addHandler(sh)

# ===== ìœ í‹¸ =====
def jitter_sleep(a=NAVER_QUERY_DELAY_RANGE[0], b=NAVER_QUERY_DELAY_RANGE[1]):
    time.sleep(random.uniform(a, b))

def sanitize_query(q: str) -> str:
    # ë”°ì˜´í‘œëŠ” ë³´ì¡´ (ì•ì„œ ë¬¸ì œì˜€ë˜ strip('"â€œâ€') ì œê±°)
    return q.strip()

def build_url(query: str) -> str:
    q = sanitize_query(query)
    return BASE_URL.format(query=quote(q))

def parse_relative_allowed(text: str) -> bool:
    t = text.strip()
    return bool(
        re.fullmatch(r"([1-9]|[1-5][0-9])\s*ë¶„\s*ì „", t)
        or re.fullmatch(r"([1-9]|1[0-9]|2[0-3])\s*ì‹œê°„\s*ì „", t)
    )

def has_classes(tag, classes) -> bool:
    return bool(tag and tag.has_attr("class") and all(c in tag["class"] for c in classes))

def normalize_title(t: str) -> str:
    return " ".join((t or "").casefold().split())

def normalize_link(url: str) -> str:
    if not url: return ""
    try:
        p = urlparse(url)
        scheme = p.scheme.lower() or "https"
        netloc = p.netloc.lower()
        if netloc.endswith("news.naver.com"):
            qs = parse_qs(p.query)
            oid = qs.get("oid", [None])[0]
            aid = qs.get("aid", [None])[0]
            if oid and aid:
                return f"naver:oid={oid}&aid={aid}"
            return f"{netloc}{p.path.rstrip('/')}"
        qs = parse_qs(p.query, keep_blank_values=True)
        qs_clean = {k: v for k, v in qs.items() if k not in TRACKING_PARAMS}
        qs_clean = {k: sorted(v) for k, v in qs_clean.items()}
        new_query = urlencode(qs_clean, doseq=True)
        new_path = p.path.rstrip("/") or "/"
        return urlunparse((scheme, netloc, new_path, "", new_query, ""))
    except Exception:
        return url

# ===== ë„¤íŠ¸ì›Œí¬ =====
def make_session(max_retry=MAX_RETRY_DEFAULT) -> requests.Session:
    s = requests.Session()
    s.headers.update({"User-Agent": random.choice(UA_POOL), **COMMON_HEADERS})
    try:
        r = s.get(HOMEPAGE, timeout=8)
        r.raise_for_status()
        logger.info("ë„¤ì´ë²„ ì¿ í‚¤ ì›Œë°ì—… ì„±ê³µ")
    except Exception as e:
        logger.info(f"ì›Œë°ì—… ì‹¤íŒ¨: {e}")
    s._max_retry = max_retry
    return s

def get_html(session: requests.Session, url: str) -> str | None:
    for attempt in range(1, getattr(session, "_max_retry", MAX_RETRY_DEFAULT) + 1):
        try:
            r = session.get(url, timeout=15, allow_redirects=True)
            if r.status_code == 200:
                return r.text
            logger.info(f"HTTP {r.status_code} (ì‹œë„ {attempt})")
        except Exception as e:
            logger.info(f"ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {attempt}): {e}")
        time.sleep(RETRY_BACKOFF_BASE ** attempt)
    return None

# ===== íŒŒì„œ =====
def extract_card_from_time_span(span):
    a = None; steps = 0
    for e in span.next_elements:
        steps += 1
        if steps > 200: break
        if isinstance(e, str): continue
        if getattr(e, "name", None) == "a" and e.get("data-heatmap-target") == ".tit":
            a = e; break
    if not a: return None
    title = a.get_text(strip=True)
    link  = a.get("href", "")

    snippet = ""
    steps = 0
    for e in a.next_elements:
        steps += 1
        if steps > 200: break
        if getattr(e, "name", None) == "span" and e.has_attr("class"):
            if any(c in e["class"] for c in SNIPPET_CLASSES):
                txt = e.get_text(" ", strip=True)
                if txt and txt != title and len(txt) >= 10:
                    snippet = txt; break
    return {"title": title, "link": link, "snippet": snippet}

# ===== ìˆ˜ì§‘ =====
def fetch_news(session: requests.Session, query: str, max_n: int, include_query_col: bool) -> list[dict]:
    url = build_url(query)
    logger.info(f"ê²€ìƒ‰: {query}")
    html = get_html(session, url)
    if not html:
        logger.info("HTML íšë“ ì‹¤íŒ¨")
        return []

    soup = BeautifulSoup(html, "html.parser")
    spans = [s for s in soup.find_all("span", class_=lambda x: x) if has_classes(s, TIME_SPAN_CLASS)]

    seen_titles, seen_links, rows = set(), set(), []
    for s in spans:
        if len(rows) >= max_n: break
        t = s.get_text(strip=True)
        if not parse_relative_allowed(t): continue
        card = extract_card_from_time_span(s)
        if not card: continue

        link_norm  = normalize_link(card["link"])
        title_norm = normalize_title(card["title"])
        if (link_norm and link_norm in seen_links) or (title_norm in seen_titles):
            continue
        if link_norm: seen_links.add(link_norm)
        seen_titles.add(title_norm)

        row = {"title": card["title"], "snippet": card["snippet"], "link": card["link"]}
        if include_query_col: row["query"] = query
        rows.append(row)
    jitter_sleep()
    return rows

# ===== ì—‘ì…€ í—¬í¼ =====
def write_sheet(ws, rows: list[dict], include_query_col: bool):
    if include_query_col:
        ws.append(["ê²€ìƒ‰ì–´", "í…ìŠ¤íŠ¸(ì œëª©)", "ê¸°ì‚¬ë‚´ìš©", "ë§í¬"])
        widths = [28, 60, 100, 80]
        ws.freeze_panes = "A2"
        for i, w in enumerate(widths, 1): ws.column_dimensions[get_column_letter(i)].width = w
        for r in rows: ws.append([r.get("query",""), r.get("title",""), r.get("snippet",""), r.get("link","")])
    else:
        ws.append(["í…ìŠ¤íŠ¸(ì œëª©)", "ê¸°ì‚¬ë‚´ìš©", "ë§í¬"])
        widths = [60, 100, 80]
        ws.freeze_panes = "A2"
        for i, w in enumerate(widths, 1): ws.column_dimensions[get_column_letter(i)].width = w
        for r in rows: ws.append([r.get("title",""), r.get("snippet",""), r.get("link","")])

def build_workbook(data_clients: list[dict], data_custom: list[dict] | None, data_market: list[dict]):
    wb = Workbook()

    # ì‹œíŠ¸1: ê³ ê°ì‚¬
    ws1 = wb.active
    ws1.title = "ê³ ê°ì‚¬"
    write_sheet(ws1, data_clients, include_query_col=True)

    # ì‹œíŠ¸2: ì‚¬ìš©ì ì§€ì • (ìˆì„ ë•Œë§Œ)
    if data_custom is not None:
        ws2 = wb.create_sheet("ì‚¬ìš©ì ì§€ì •")
        write_sheet(ws2, data_custom, include_query_col=True)

    # ì‹œíŠ¸3: ì „ë ¥ì‹œì¥ ë™í–¥
    ws3 = wb.create_sheet("ì „ë ¥ì‹œì¥ ë™í–¥")
    write_sheet(ws3, data_market, include_query_col=False)

    bio = io.BytesIO()
    wb.save(bio)
    bio.seek(0)
    return bio

# ===== Streamlit UI =====
st.set_page_config(page_title="ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°", layout="wide")
st.title("ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° (v6 + Streamlit)")

with st.sidebar:
    st.header("ì„¤ì •")
    max_n = st.slider("ìµœëŒ€ ìˆ˜ì§‘ ìˆ˜ (ìœ ë‹ˆí¬)", 1, 10, 10)
    custom_mode = st.checkbox("ì‚¬ìš©ì ì§€ì • ê²€ìƒ‰ì–´ ëª¨ë“œ", value=False, help='í…ìŠ¤íŠ¸ë°•ìŠ¤ì— ì¤„ ë‹¨ìœ„ë¡œ ê²€ìƒ‰ì–´ë¥¼ ì§ì ‘ ì…ë ¥í•©ë‹ˆë‹¤.')
    include_market = st.checkbox("ì „ë ¥ì‹œì¥ +ì—ë„ˆì§€ ìˆ˜ì§‘", value=True)
    uploaded = st.file_uploader("ê³ ê°ì‚¬ ì—‘ì…€ ì—…ë¡œë“œ (Aì—´)", type=["xlsx"])
    st.caption("â€» ê³ ê°ì‚¬/ì‚¬ìš©ì ì§€ì •ì€ ë‘˜ ë‹¤ ì²´í¬ ì‹œ **í•©ì³ì„œ** ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    run_btn = st.button("ê²€ìƒ‰ ì‹œì‘")

# ì…ë ¥ ì˜ì—­
custom_queries = []
if custom_mode:
    st.subheader("ì‚¬ìš©ì ì§€ì • ê²€ìƒ‰ì–´ ì…ë ¥")
    seed = st.text_area("ê²€ìƒ‰ì–´ë¥¼ ì¤„ ë‹¨ìœ„ë¡œ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: \"í™ˆí”ŒëŸ¬ìŠ¤\" +ì‚¬ê³ )", height=140)
    if seed.strip():
        custom_queries = [line.strip() for line in seed.splitlines() if line.strip()]

# ì‹¤í–‰
if run_btn:
    session = make_session()
    clients_rows_all: list[dict] = []
    custom_rows_all: list[dict] | None = [] if custom_mode else None
    market_rows_all: list[dict] = []

    # ê³ ê°ì‚¬ ì¿¼ë¦¬
    client_queries = []
    if uploaded is not None:
        try:
            df = pd.read_excel(uploaded, header=None)
            col = df.iloc[:, 0].dropna().astype(str).str.strip()
            # "ê°’" +ì‚¬ê³  í˜•íƒœë¡œ ìƒì„± (ë”°ì˜´í‘œ ìœ ì§€)
            client_queries = [f'"{v}" +ì‚¬ê³ ' for v in col if v]
        except Exception as e:
            st.error(f"ì—‘ì…€ ì½ê¸° ì‹¤íŒ¨: {e}")

    # ì‚¬ìš©ì ì§€ì • ì¿¼ë¦¬
    if custom_mode and custom_queries:
        # ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì˜ˆ: ì´ë¯¸ " +ì‚¬ê³ " í¬í•¨í•œ ìƒíƒœë¡œ ì…ë ¥í–ˆë‹¤ê³  ê°€ì •)
        pass

    # ê³ ê°ì‚¬ ì‹¤í–‰
    if client_queries:
        st.info(f"ê³ ê°ì‚¬ {len(client_queries)}ê±´ ìˆ˜ì§‘ ì¤‘â€¦")
        progress = st.progress(0.0)
        for idx, q in enumerate(client_queries, start=1):
            rows = fetch_news(session, q, max_n=max_n, include_query_col=True)
            clients_rows_all.extend(rows)
            progress.progress(idx / max(1, len(client_queries)))
        st.success(f"ê³ ê°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ: {len(clients_rows_all)}ê±´")

    # ì‚¬ìš©ì ì§€ì • ì‹¤í–‰
    if custom_mode and custom_queries:
        st.info(f"ì‚¬ìš©ì ì§€ì • {len(custom_queries)}ê±´ ìˆ˜ì§‘ ì¤‘â€¦")
        progress = st.progress(0.0)
        for idx, q in enumerate(custom_queries, start=1):
            rows = fetch_news(session, q, max_n=max_n, include_query_col=True)
            custom_rows_all.extend(rows)
            progress.progress(idx / max(1, len(custom_queries)))
        st.success(f"ì‚¬ìš©ì ì§€ì • ìˆ˜ì§‘ ì™„ë£Œ: {len(custom_rows_all)}ê±´")

    # ì „ë ¥ì‹œì¥ ë™í–¥
    if include_market:
        st.info("ì „ë ¥ì‹œì¥ ë™í–¥ ìˆ˜ì§‘ ì¤‘â€¦")
        market_rows_all = fetch_news(session, "ì „ë ¥ì‹œì¥ +ì—ë„ˆì§€", max_n=max_n, include_query_col=False)
        st.success(f"ì „ë ¥ì‹œì¥ ë™í–¥ ìˆ˜ì§‘ ì™„ë£Œ: {len(market_rows_all)}ê±´")

    # í‘œì‹œ & ë‹¤ìš´ë¡œë“œ
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("ê³ ê°ì‚¬ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°")
        df_clients = pd.DataFrame(clients_rows_all) if clients_rows_all else pd.DataFrame(columns=["query","title","snippet","link"])
        st.dataframe(df_clients)

    with col2:
        st.subheader("ì „ë ¥ì‹œì¥ ë™í–¥ ë¯¸ë¦¬ë³´ê¸°")
        df_market = pd.DataFrame(market_rows_all) if market_rows_all else pd.DataFrame(columns=["title","snippet","link"])
        st.dataframe(df_market)

    if custom_mode:
        st.subheader("ì‚¬ìš©ì ì§€ì • ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°")
        df_custom = pd.DataFrame(custom_rows_all) if custom_rows_all else pd.DataFrame(columns=["query","title","snippet","link"])
        st.dataframe(df_custom)

    # ì—‘ì…€ ë‹¤ìš´ë¡œë“œ
    bio = build_workbook(
        data_clients=clients_rows_all,
        data_custom=custom_rows_all if custom_mode else None,
        data_market=market_rows_all
    )
    out_name = f"ê¸°ì‚¬ìˆ˜ì§‘_{datetime.now().strftime('%Y%m%d')}.xlsx"
    st.download_button(
        label="ğŸ“¥ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
        data=bio.getvalue(),
        file_name=out_name,
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
    )



